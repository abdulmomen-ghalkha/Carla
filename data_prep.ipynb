{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da95b1a7-7a61-4840-9928-0c8c8735dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import pickle\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from tqdm import tqdm\n",
    "\n",
    "from numpy.random import RandomState\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import argparse\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fa0ee4-d42e-491f-aa03-22207d046f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#### Input dataset name\n",
    "###############################################\n",
    "root_folder = './datasets/scenario23_dev/'\n",
    "data_csv = root_folder + 'scenario23.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Read dataset to create a list of the input sequence   \n",
    "###############################################\n",
    "\n",
    "df = pd.read_csv(data_csv)\n",
    "image_data_lst = df['unit1_rgb'].values\n",
    "pwr_data_lst = df['unit1_pwr_60ghz'].values\n",
    "#original_beam = df['unit1_beam_index'].values\n",
    "\n",
    "\n",
    "###############################################\n",
    "#### subsample the power and generate the \n",
    "#### updated beam indices\n",
    "###############################################\n",
    "updated_beam = []\n",
    "original_beam = []\n",
    "for entry in pwr_data_lst:\n",
    "    data_to_read = f'./{root_folder}{entry[1:]}'\n",
    "    pwr_data = np.loadtxt(data_to_read)\n",
    "    original_beam.append(np.argmax(pwr_data)+1)\n",
    "    updated_pwr = []\n",
    "    j = 0\n",
    "    while j < (len(pwr_data)- 1):\n",
    "        tmp_pwr = pwr_data[j]\n",
    "        updated_pwr.append(tmp_pwr)\n",
    "        j += 2\n",
    "    updated_beam.append(np.argmax(updated_pwr)+1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5263b2d8-876c-4a90-b67e-8bf4d05a2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the image based dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11387/11387 [00:00<00:00, 2186837.90it/s]\n",
      "C:\\Users\\abdu_\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the position based dataset\n",
      "Reading the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11387/11387 [01:24<00:00, 134.27it/s]\n",
      "C:\\Users\\abdu_\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def create_img_beam_dataset():   \n",
    "    \n",
    "    folder_to_save = 'image_beam'\n",
    "    if not os.path.exists(folder_to_save):\n",
    "        os.makedirs(folder_to_save)\n",
    "    \n",
    "    #############################################\n",
    "    ###### created updated image path ###########\n",
    "    #############################################\n",
    "    updated_img_path = []\n",
    "    print(\"Creating the image based dataset\")\n",
    "    for entry in tqdm(image_data_lst):\n",
    "        img_path = entry.split('./')[1]\n",
    "        updated_path = f'./{root_folder}/{img_path}'\n",
    "        updated_img_path.append(updated_path)\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # saving the image-beam development dataset for training and validation\n",
    "    #############################################\n",
    "                            \n",
    "    indx = np.arange(1, len(updated_beam)+1,1)\n",
    "    df_new = pd.DataFrame()\n",
    "    df_new['index'] = indx   \n",
    "    df_new['unit1_rgb'] = updated_img_path   \n",
    "    df_new['unit1_beam'] = updated_beam    \n",
    "    df_new.to_csv(fr'./{folder_to_save}/scenario23_img_beam.csv', index=False)       \n",
    "      \n",
    "    #############################################\n",
    "    #generate the train and test dataset\n",
    "    #############################################    \n",
    "    rng = RandomState(1)\n",
    "    train, val, test = np.split(df_new.sample(frac=1, random_state=rng ), [int(.6*len(df_new)), int(.9*len(df_new))])\n",
    "    train.to_csv(f'./{folder_to_save}/scenario23_img_beam_train.csv', index=False)\n",
    "    val.to_csv(f'./{folder_to_save}/scenario23_img_beam_val.csv', index=False)\n",
    "    test.to_csv(f'./{folder_to_save}/scenario23_img_beam_test.csv', index=False)\n",
    "\n",
    "\n",
    "def create_pos_beam_dataset():  \n",
    "    \n",
    "    folder_to_save = './pos_beam'\n",
    "    if not os.path.exists(folder_to_save):\n",
    "        os.makedirs(folder_to_save)\n",
    "    \n",
    "    ###############################################\n",
    "    ####### read position values from dataset #####\n",
    "    ###############################################\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    pos_data_path = df['unit2_loc'].values\n",
    "    print(\"Creating the position based dataset\")\n",
    "    print(\"Reading the dataset\")\n",
    "    for entry in tqdm(pos_data_path):\n",
    "        data_to_read = f'./{root_folder}{entry[1:]}'\n",
    "        pos_val = np.loadtxt(data_to_read)\n",
    "        #lat_val, lon_val = pos_val[0], pos_val[1]\n",
    "        lat.append(pos_val[0])\n",
    "        lon.append(pos_val[1])\n",
    "        \n",
    "    def norm_data(data_lst):\n",
    "        norm_data = []\n",
    "        for entry in data_lst:\n",
    "            norm_data.append((entry - min(data_lst))/(max(data_lst) - min(data_lst)))\n",
    "        return norm_data\n",
    "\n",
    "    ###############################################\n",
    "    ##### normalize latitude and longitude data ###\n",
    "    ###############################################\n",
    "    lat_norm = norm_data(lat)\n",
    "    lon_norm = norm_data(lon)\n",
    "\n",
    "    ###############################################\n",
    "    ##### generate final pos data #################\n",
    "    ###############################################\n",
    "    pos_data = []\n",
    "    for j in range(len(lat_norm)):\n",
    "        pos_data.append([lat_norm[j], lon_norm[j]])\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # saving the pos-beam development dataset for training and validation\n",
    "    #############################################\n",
    "                            \n",
    "    indx = np.arange(1, len(updated_beam)+1,1)\n",
    "    df_new = pd.DataFrame()\n",
    "    df_new['index'] = indx   \n",
    "    df_new['unit2_pos'] = pos_data   \n",
    "    df_new['unit1_beam'] = updated_beam    \n",
    "    df_new.to_csv(fr'./{folder_to_save}/scenario23_pos_beam.csv', index=False) \n",
    "    \n",
    "    #############################################\n",
    "    #generate the train and test dataset\n",
    "    #############################################    \n",
    "    rng = RandomState(1)\n",
    "    train, val, test = np.split(df_new.sample(frac=1, random_state=rng ), [int(.6*len(df_new)), int(.9*len(df_new))])\n",
    "    train.to_csv(f'./{folder_to_save}/scenario23_pos_beam_train.csv', index=False)\n",
    "    val.to_csv(f'./{folder_to_save}/scenario23_pos_beam_val.csv', index=False)\n",
    "    test.to_csv(f'./{folder_to_save}/scenario23_pos_beam_test.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_img_beam_dataset()\n",
    "create_pos_beam_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51c172b-cd5e-44ca-b180-211c958a99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the position based dataset\n",
      "Reading the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11387/11387 [00:02<00:00, 4319.82it/s]\n",
      "100%|██████████| 11387/11387 [01:43<00:00, 109.73it/s]\n",
      "C:\\Users\\abdu_\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def create_pos_height_beam_dataset():  \n",
    "    \n",
    "    folder_to_save = './pos_height_beam'\n",
    "    if not os.path.exists(folder_to_save):\n",
    "        os.makedirs(folder_to_save)\n",
    "    \n",
    "    ###############################################\n",
    "    ####### read position values from dataset #####\n",
    "    ###############################################\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    height = []\n",
    "    distance = []\n",
    "    \n",
    "    pos_data_path = df['unit2_loc'].values\n",
    "    height_data_path = df['unit2_height'].values\n",
    "    distance_data_path = df['unit2_distance'].values\n",
    "    print(\"Creating the position based dataset\")\n",
    "    print(\"Reading the dataset\")\n",
    "    for entry in tqdm(pos_data_path):\n",
    "        data_to_read = f'./{root_folder}{entry[1:]}'\n",
    "        pos_val = np.loadtxt(data_to_read)\n",
    "        #lat_val, lon_val = pos_val[0], pos_val[1]\n",
    "        lat.append(pos_val[0])\n",
    "        lon.append(pos_val[1])\n",
    "        \n",
    "    for entry_height, entry_distance in zip(tqdm(height_data_path), distance_data_path) :\n",
    "        data_to_read_height = f'./{root_folder}{entry_height[1:]}'\n",
    "        data_to_read_distance = f'./{root_folder}{entry_distance[1:]}'\n",
    "        \n",
    "        height_val = np.loadtxt(data_to_read_height)\n",
    "        distance_val = np.loadtxt(data_to_read_distance)\n",
    "        height.append(height_val)\n",
    "        distance.append(distance_val)\n",
    "        \n",
    "        \n",
    "    def norm_data(data_lst):\n",
    "        norm_data = []\n",
    "        for entry in data_lst:\n",
    "            norm_data.append((entry - min(data_lst))/(max(data_lst) - min(data_lst)))\n",
    "        return norm_data\n",
    "\n",
    "    ###############################################\n",
    "    ##### normalize latitude and longitude data ###\n",
    "    ###############################################\n",
    "    lat_norm = norm_data(lat)\n",
    "    lon_norm = norm_data(lon)\n",
    "    height_norm = norm_data(height)\n",
    "    distance_norm = norm_data(distance)\n",
    "    ###############################################\n",
    "    ##### generate final pos data #################\n",
    "    ###############################################\n",
    "    #pos_data = []\n",
    "    #for j in range(len(lat_norm)):\n",
    "    #    pos_data.append([lat_norm[j], lon_norm[j]])\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # saving the pos-beam development dataset for training and validation\n",
    "    #############################################\n",
    "                            \n",
    "    indx = np.arange(1, len(updated_beam)+1,1)\n",
    "    df_new = pd.DataFrame()\n",
    "    df_new['index'] = indx   \n",
    "    df_new['unit2_lat'] = lat_norm\n",
    "    df_new['unit2_lon'] = lon_norm\n",
    "    df_new['unit2_height'] = height_norm\n",
    "    df_new['unit2_distance'] = distance_norm\n",
    "    df_new['unit1_beam'] = updated_beam    \n",
    "    df_new.to_csv(fr'./{folder_to_save}/scenario23_pos_height_beam.csv', index=False) \n",
    "    \n",
    "    #############################################\n",
    "    #generate the train and test dataset\n",
    "    #############################################    \n",
    "    rng = RandomState(1)\n",
    "    train, val, test = np.split(df_new.sample(frac=1, random_state=rng ), [int(.6*len(df_new)), int(.9*len(df_new))])\n",
    "    train.to_csv(f'./{folder_to_save}/scenario23_pos_height_beam_train.csv', index=False)\n",
    "    val.to_csv(f'./{folder_to_save}/scenario23_pos_height_beam_val.csv', index=False)\n",
    "    test.to_csv(f'./{folder_to_save}/scenario23_pos_height_beam_test.csv', index=False)   \n",
    "\n",
    "\n",
    "\n",
    "create_pos_height_beam_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c606dc5-b32d-42b8-9bde-21a7ab4bd902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "././datasets/scenario23_dev//unit1/mmWave_data/mmWave_power_1.txt\n",
      "Exist\n",
      "././datasets/scenario23_dev//unit1/mmWave_data/mmWave_power_2.txt\n",
      "Exist\n",
      "././datasets/scenario23_dev//unit1/mmWave_data/mmWave_power_3.txt\n",
      "Exist\n",
      "././datasets/scenario23_dev//unit1/mmWave_data/mmWave_power_4.txt\n",
      "Exist\n",
      "././datasets/scenario23_dev//unit1/mmWave_data/mmWave_power_5.txt\n",
      "Exist\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#### Input dataset name\n",
    "###############################################\n",
    "root_folder = './datasets/scenario23_dev/'\n",
    "data_csv = root_folder + 'scenario23.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Read dataset to create a list of the input sequence   \n",
    "###############################################\n",
    "\n",
    "df = pd.read_csv(data_csv)\n",
    "image_data_lst = df['unit1_rgb'].values\n",
    "pwr_data_lst = df['unit1_pwr_60ghz'].values\n",
    "#original_beam = df['unit1_beam_index'].values\n",
    "\n",
    "\n",
    "###############################################\n",
    "#### subsample the power and generate the \n",
    "#### updated beam indices\n",
    "###############################################\n",
    "updated_beam = []\n",
    "original_beam = []\n",
    "for num, entry in zip(range(5), pwr_data_lst):\n",
    "    data_to_read = f'./{root_folder}{entry[1:]}'\n",
    "    pwr_data = np.loadtxt(data_to_read)\n",
    "    original_beam.append(np.argmax(pwr_data)+1)\n",
    "    updated_pwr = []\n",
    "    j = 0\n",
    "    print(data_to_read)\n",
    "    if os.path.exists(data_to_read):\n",
    "        print(\"Exist\")\n",
    "    while j < (len(pwr_data)- 1):\n",
    "        tmp_pwr = pwr_data[j]\n",
    "        updated_pwr.append(tmp_pwr)\n",
    "        j += 2\n",
    "    updated_beam.append(np.argmax(updated_pwr)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7309a819-f4b1-4470-ae0f-682bc0068ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the image based dataset\n",
      "unit1/camera_data/image_BS1_1_16_58_42.jpg\n",
      "././datasets/scenario23_dev//unit1/camera_data/image_BS1_1_16_58_42.jpg\n",
      "Exist\n",
      "unit1/camera_data/image_BS1_2_16_58_42.jpg\n",
      "././datasets/scenario23_dev//unit1/camera_data/image_BS1_2_16_58_42.jpg\n",
      "Exist\n",
      "unit1/camera_data/image_BS1_3_16_58_42.jpg\n",
      "././datasets/scenario23_dev//unit1/camera_data/image_BS1_3_16_58_42.jpg\n",
      "Exist\n",
      "unit1/camera_data/image_BS1_4_16_58_42.jpg\n",
      "././datasets/scenario23_dev//unit1/camera_data/image_BS1_4_16_58_42.jpg\n",
      "Exist\n",
      "unit1/camera_data/image_BS1_5_16_58_42.jpg\n",
      "././datasets/scenario23_dev//unit1/camera_data/image_BS1_5_16_58_42.jpg\n",
      "Exist\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_csv)\n",
    "image_data_lst = df['unit1_rgb'].values\n",
    "pwr_data_lst = df['unit1_pwr_60ghz'].values\n",
    "\n",
    "updated_img_path = []\n",
    "print(\"Creating the image based dataset\")\n",
    "for num, entry in zip(range(5), image_data_lst):\n",
    "    img_path = entry.split('./')[1]\n",
    "    updated_path = f'./{root_folder}/{img_path}'\n",
    "    updated_img_path.append(updated_path)\n",
    "    print(img_path)\n",
    "    print(updated_path)\n",
    "    if os.path.exists(updated_path):\n",
    "        print(\"Exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39a9b1-880d-4c0d-bcbe-c44c2e7ce3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
