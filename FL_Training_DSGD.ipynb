{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29303e79-2191-48c2-88cf-d4f8f97499d9",
   "metadata": {},
   "source": [
    "# Scenario 1: All modalities exists, no heterginity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4f0f1d-2404-4acf-8593-99c905a32a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression, resnet18_mod\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6ca09e-bc32-4f5f-8b1e-f27a82f12f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! PyTorch can use the GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! PyTorch can use the GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9856d27-3d53-43f2-91fa-380f7b7fb8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "lr = 1e-3\n",
    "decay = 1e-4\n",
    "num_epochs = 20\n",
    "train_size = [1]\n",
    "no_users = 20\n",
    "\n",
    "val_losses_stand_alone = []\n",
    "val_losses_FL = []\n",
    "val_losses_SFMTL = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2aff42-b785-4b9f-9c54-59cef3e4af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded_user: 0\n",
      "Loaded_user: 1\n",
      "Loaded_user: 2\n",
      "Loaded_user: 3\n",
      "Loaded_user: 4\n",
      "Loaded_user: 5\n",
      "Loaded_user: 6\n",
      "Loaded_user: 7\n",
      "Loaded_user: 8\n",
      "Loaded_user: 9\n",
      "Loaded_user: 10\n",
      "Loaded_user: 11\n",
      "Loaded_user: 12\n",
      "Loaded_user: 13\n",
      "Loaded_user: 14\n",
      "Loaded_user: 15\n",
      "Loaded_user: 16\n",
      "Loaded_user: 17\n",
      "Loaded_user: 18\n",
      "Loaded_user: 19\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "dataset_dir = \"feature_IID/\"\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "for user_id in range(no_users):\n",
    "    train_dir = dataset_dir + f'user_{user_id}_pos_height_beam_train.csv'\n",
    "    val_dir = dataset_dir + f'user_{user_id}_pos_height_beam_val.csv'\n",
    "    test_dir = dataset_dir + f'user_{user_id}_pos_height_beam_test.csv'\n",
    "    \n",
    "    train_dataset = DataFeed_image_pos(train_dir, transform=proc_pipe)\n",
    "    val_dataset = DataFeed_image_pos(root_dir=val_dir, transform=proc_pipe)\n",
    "    test_dataset = DataFeed_image_pos(root_dir=test_dir, transform=proc_pipe)\n",
    "    \n",
    "    \n",
    "    train_loaders.append(DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                              shuffle=False))\n",
    "    val_loaders.append(DataLoader(val_dataset,\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    test_loaders.append(DataLoader(test_dataset,\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    print(f\"Loaded_user: {user_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d431fabe-5cae-456a-9ecb-10342010679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "for _, (x, y) in zip(range(1), train_loaders[0]):\n",
    "    print(x[\"pos_height\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e09dd8-16e0-4181-949e-8c829098993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "user_modalities = [available_modalities for _ in range(no_users)]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "output_sizes = [sum([modality_size[i] for i in user_modality]) for user_modality in user_modalities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33e6696-0636-4b3f-bd6e-d84baacd3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix for 'pos_height':\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "Similarity Matrix for 'images':\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store similarity matrices for each modality\n",
    "similarity_matrices = {modality: np.zeros((no_users, no_users), dtype=int) for modality in available_modalities}\n",
    "\n",
    "# Populate the similarity matrices\n",
    "for modality in available_modalities:\n",
    "    for i in range(no_users):\n",
    "        for j in range(no_users):\n",
    "            if modality in user_modalities[i] and modality in user_modalities[j]:\n",
    "                similarity_matrices[modality][i, j] = 1\n",
    "\n",
    "# Print the resulting matrices\n",
    "for modality, matrix in similarity_matrices.items():\n",
    "    print(f\"Similarity Matrix for '{modality}':\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16a6d65-dc6d-4352-a597-d37acd1554ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        if nx.is_connected(graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1345c76-333e-4180-b438-4466eed22669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random connected topology\n",
    "adjacency_matrix = create_random_topology(no_users, edge_probability=0.3)\n",
    "\n",
    "# Prepare mixing matrices for each modality\n",
    "mixing_matrices, adjacency_matrices = prepare_mixing_matrices(adjacency_matrix, similarity_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093577f9-b3f0-4713-91b9-ef34f2ab76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Decentralized aggregation function\n",
    "def decentralized_aggregation(user_models, mixing_matrices, available_modalities):\n",
    "    num_users = len(user_models)\n",
    "    \n",
    "    for modality in available_modalities:\n",
    "        # Get the mixing matrix for the current modality\n",
    "        mixing_matrix = mixing_matrices[modality]\n",
    "        \n",
    "        # Convert user model parameters to vectors for aggregation\n",
    "        aggregated_models = [torch.nn.utils.parameters_to_vector(user_model[modality].parameters()) for user_model in user_models]\n",
    "        \n",
    "        # Initialize aggregated updates\n",
    "        aggregated_updates = [torch.zeros_like(aggregated_models[0]) for _ in range(num_users)]\n",
    "        \n",
    "        # Perform model aggregation based on the mixing matrix for this modality\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_users):\n",
    "                if mixing_matrix[i, j] > 0:\n",
    "                    aggregated_updates[i] += mixing_matrix[i, j] * aggregated_models[j]\n",
    "        \n",
    "        # Update user models with aggregated parameters for the current modality\n",
    "        for i in range(num_users):\n",
    "            torch.nn.utils.vector_to_parameters(aggregated_updates[i], user_models[i][modality].parameters())\n",
    "\n",
    "\n",
    "def train_local_model(local_modalities, models, train_loader, criterion, optimizers, epochs):\n",
    "\n",
    "    for modality in local_modalities:\n",
    "        print(f\"Training for modality: {modality}\")\n",
    "        \n",
    "        model = models[modality]\n",
    "        optimizer = optimizers[modality]\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for data, labels in train_loader:\n",
    "                # Move data to GPU if available\n",
    "                data = data[modality]\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                temp, outputs = model(data)\n",
    "                print(temp.shape)\n",
    "\n",
    "               \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Optional: Print loss for debugging\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Modality: {modality}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a079548-3c7c-46bb-bfc1-be6ad5984a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_models(user_id, user_models, val_loaders, criterion):\n",
    "\n",
    "    print(f\"Validating model for User {user_id + 1}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for modality, model in user_models.items():\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            model.eval()\n",
    "            if modality not in user_models.keys():\n",
    "                print(f\"Skipping modality {modality} for User {user_id + 1}, no validation data.\")\n",
    "                continue\n",
    "            \n",
    "            for data, labels in val_loaders:  # Iterate over validation data for the modality\n",
    "                data = data[modality]\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                # Forward pass\n",
    "                _, outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate loss and accuracy\n",
    "                total_loss += loss.item() * labels.size(0)  # Sum loss for the batch\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "            accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "            print(f\"User {user_id + 1}, modality: {modality} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return {'loss': avg_loss, 'accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be75bb1-af67-4841-8422-434bddf7913f",
   "metadata": {},
   "source": [
    "# Decentralized FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c3ca8a8-151f-487e-b619-80692ac78789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "user_modalities = [available_modalities for _ in range(no_users)]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "output_sizes = [sum([modality_size[i] for i in user_modality]) for user_modality in user_modalities]\n",
    "\n",
    "\n",
    "for user_id in range(no_users):\n",
    "    user_model = {}\n",
    "    if \"images\" in user_modalities[user_id]:\n",
    "        user_model[\"images\"] = resnet50(pretrained=True, progress=True, num_classes=64).cuda()\n",
    "    if \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model[\"pos_height\"] = NN_beam_pred(num_features=4, num_output=64).cuda()\n",
    "    all_models.append(user_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb99571-7241-4590-bf2c-5cb0bc13cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round 1\n",
      "Training image modalitity models\n",
      "Training model for User 1\n",
      "Training for modality: pos_height\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1702\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1614\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1562\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1455\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1427\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1307\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1277\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1191\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1088\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1000\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0690\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0701\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0450\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0088\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.9800\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.9327\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.8906\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.8669\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.7966\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.7586\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.6612\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.6258\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.6063\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.4178\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.2460\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.2785\n",
      "torch.Size([50, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.2777\n",
      "Training for modality: images\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 4.1483\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 3.3954\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 3.0845\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.9108\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.8504\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.7126\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.5806\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.5061\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.1733\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.4547\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.1150\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.4516\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.1976\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.7832\n",
      "torch.Size([64, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m modality \u001b[38;5;129;01min\u001b[39;00m user_model\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     17\u001b[0m         optimizers[modality] \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(user_models[modality]\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mtrain_local_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_modalities\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Decentralized aggregation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming decentralized aggregation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m, in \u001b[0;36mtrain_local_model\u001b[1;34m(local_modalities, models, train_loader, criterion, optimizers, epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;66;03m# Optional: Print loss for debugging\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Modality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "local_epochs = 1\n",
    "global_rounds = 50\n",
    "weight_decay = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Decentralized Training\n",
    "for round_num in range(global_rounds):\n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Training for image_modalities \n",
    "    print(\"Training image modalitity models\")\n",
    "    # Train each user's local model\n",
    "    for user_id in range(no_users):\n",
    "        print(f\"Training model for User {user_id + 1}\")\n",
    "        user_models = all_models[user_id]\n",
    "        optimizers = {}\n",
    "        for modality in user_model.keys():\n",
    "            optimizers[modality] = optim.Adam(user_models[modality].parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train_local_model(user_modalities[user_id], user_models, train_loaders[user_id], criterion, optimizers, local_epochs)\n",
    "\n",
    "    # Decentralized aggregation\n",
    "    print(\"Performing decentralized aggregation\")\n",
    "    decentralized_aggregation(all_models, mixing_matrices, available_modalities)\n",
    "\n",
    "    # Optionally, validate models\n",
    "    print(\"Validating user models...\")\n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        val_dict = validate_user_models(user_id, user_models, val_loaders[user_id], criterion)\n",
    "        val_losses_FL.append(val_dict)\n",
    "\n",
    "\n",
    "print(\"Decentralized federated learning complete.\")\n",
    "\n",
    "file_path = 'val_losses_FL.json'\n",
    "\n",
    "# Save the list of dictionaries to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(val_losses_FL, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c2fc9-8e70-4b8a-b028-5a12a82cb7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03250a-0d1a-4468-bafa-95eac4dc6654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
