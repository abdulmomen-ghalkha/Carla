{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba7f2e2-31fd-4d6e-9fc8-a7eace461083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import torch as t\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_feed import DataFeed\n",
    "from build_net import resnet50\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46ac1e4-c6b9-49ad-9f73-d08cdcedcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-30-2024\n",
      "18_20\n",
      "C:\\Users\\aghalkha21\\Downloads\\Git_Projects\\Carla//saved_folder//12-30-2024_18_20\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "########### Create save directory ##########\n",
    "############################################\n",
    "\n",
    "# year month day \n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds \n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)\n",
    "dataset_dir = './image_beam/'\n",
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime \n",
    "print(pwd)\n",
    "# Determine whether the folder already exists\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)    \n",
    "    \n",
    "\n",
    "#copy the training files to the saved directory\n",
    "shutil.copy('./training_rgb.ipynb', pwd)\n",
    "shutil.copy('./data_feed.py', pwd)\n",
    "shutil.copy('./build_net.py', pwd)\n",
    "shutil.copy(dataset_dir + 'scenario23_img_beam_train.csv', pwd)\n",
    "shutil.copy(dataset_dir + 'scenario23_img_beam_val.csv', pwd)\n",
    "shutil.copy(dataset_dir +'scenario23_img_beam_test.csv', pwd)\n",
    "\n",
    "\n",
    "#create folder to save analysis files and checkpoint\n",
    "\n",
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory) \n",
    "    \n",
    "isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)         \n",
    "\n",
    "############################################    \n",
    "\n",
    "########################################################################\n",
    "######################### Hyperparameters ##############################\n",
    "########################################################################\n",
    "\n",
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "lr = 1e-3\n",
    "decay = 1e-4\n",
    "num_epochs = 20\n",
    "train_size = [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d5d937-1175-4b78-9afa-ef5807cd54b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```````````````````````````````````````````````````````\n",
      "Training size is 1\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Epoch No. 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'C:\\Users\\aghalkha21\\Downloads\\Git_Projects\\datasets\\scenario23_dev\\unit1\\camera_data\\image_BS1_3532_17_08_22.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch No. \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     56\u001b[0m skipped_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tr_count, (img, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     58\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m img\u001b[38;5;66;03m#.cuda()\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Downloads\\Git_Projects\\Carla\\data_feed.py:45\u001b[0m, in \u001b[0;36mDataFeed.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     44\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n\u001b[1;32m---> 45\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     47\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\skimage\\io\\_io.py:60\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     57\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname:\n\u001b[1;32m---> 60\u001b[0m     img \u001b[38;5;241m=\u001b[39m call_plugin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimread\u001b[39m\u001b[38;5;124m'\u001b[39m, fname, plugin\u001b[38;5;241m=\u001b[39mplugin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\skimage\\io\\manage_plugins.py:217\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(imageio_imread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRITEABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     13\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\imageio\\v3.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_kwargs) \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(img_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\imageio\\core\\imopen.py:113\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request\u001b[38;5;241m.\u001b[39mformat_hint \u001b[38;5;241m=\u001b[39m format_hint\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bytes>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\imageio\\core\\request.py:247\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\imageio\\core\\request.py:407\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fn):\n\u001b[1;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m fn)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     dn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(fn)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'C:\\Users\\aghalkha21\\Downloads\\Git_Projects\\datasets\\scenario23_dev\\unit1\\camera_data\\image_BS1_3532_17_08_22.jpg'"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dir = dataset_dir + 'scenario23_img_beam_train.csv'\n",
    "val_dir = dataset_dir + 'scenario23_img_beam_val.csv'\n",
    "train_loader = DataLoader(DataFeed(train_dir, transform=proc_pipe),\n",
    "                          batch_size=batch_size,\n",
    "                          #num_workers=8,\n",
    "                          shuffle=False)\n",
    "val_loader = DataLoader(DataFeed(val_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "with cuda.device(-1):\n",
    "   \n",
    "    acc_loss = 0\n",
    "    itr = []\n",
    "    for idx, n in enumerate(train_size):\n",
    "        print('```````````````````````````````````````````````````````')\n",
    "        print('Training size is {}'.format(n))\n",
    "        # Build the network:\n",
    "        net = resnet50(pretrained=True, progress=True, num_classes=64)\n",
    "        net = net#.cuda()\n",
    "        #summary(net, (3, 224, 224)) #.cuda()\n",
    "\n",
    "        #  Optimization parameters:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = optimizer.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [4,8,12], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "        count = 0\n",
    "        running_loss = []\n",
    "        running_top1_acc = []\n",
    "        running_top2_acc = []\n",
    "        running_top3_acc = []\n",
    "        running_top5_acc = []\n",
    "        \n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch No. ' + str(epoch + 1))\n",
    "            skipped_batches = 0\n",
    "            for tr_count, (img, label) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                x = img#.cuda()\n",
    "                opt.zero_grad()\n",
    "                label = label#.cuda()\n",
    "                _, out = net.forward(x)\n",
    "                L = criterion(out, label)\n",
    "                L.backward()\n",
    "                opt.step()\n",
    "                batch_loss = L.item()\n",
    "                acc_loss += batch_loss\n",
    "                count += 1\n",
    "                if np.mod(count, 10) == 0:\n",
    "                    print('Training-Batch No.' + str(count))\n",
    "                    running_loss.append(batch_loss)  # running_loss.append()\n",
    "                    itr.append(count)\n",
    "                    print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "            print('Start validation')\n",
    "            ave_top1_acc = 0\n",
    "            ave_top2_acc = 0\n",
    "            ave_top3_acc = 0\n",
    "            ave_top5_acc = 0\n",
    "            ind_ten = t.as_tensor([0, 1, 2, 3, 4], device='cuda:-1')\n",
    "            top1_pred_out = []\n",
    "            top2_pred_out = []\n",
    "            top3_pred_out = []\n",
    "            top5_pred_out = []\n",
    "            gt_beam = []\n",
    "            total_count = 0\n",
    "            for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "                net.eval()\n",
    "                x = imgs#.cuda()\n",
    "                opt.zero_grad()\n",
    "                labels = labels#.cuda()\n",
    "                total_count += labels.size(0)\n",
    "                _, out = net.forward(x)\n",
    "                _, top_1_pred = t.max(out, dim=1)\n",
    "                \n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "                \n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "                sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "                \n",
    "                top_2_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:2])\n",
    "                top2_pred_out.append(top_2_pred.detach().cpu().numpy()[0])\n",
    "                \n",
    "                top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "                \n",
    "                top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten)\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])                      \n",
    "                \n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_2_labels = reshaped_labels.repeat(1, 2)\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5) \n",
    "                \n",
    "                batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "                batch_top2_acc = t.sum(top_2_pred == tiled_2_labels, dtype=t.float32)\n",
    "                batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "                batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)                    \n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top2_acc += batch_top2_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()                    \n",
    "            print(\"total test examples are\", total_count)\n",
    "            running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "            running_top2_acc.append(ave_top2_acc / total_count)\n",
    "            running_top3_acc.append(ave_top3_acc / total_count)  # (batch_size * (count_2 + 1)))\n",
    "            running_top5_acc.append(ave_top5_acc / total_count)  # (batch_size * (count_2 + 1)))                \n",
    "            print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "            print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "            print('Average Top-2 accuracy {}'.format( running_top2_acc[-1]))\n",
    "            print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "            print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))                \n",
    "\n",
    "            \n",
    "            cur_accuracy  = running_top1_acc[-1]\n",
    "\n",
    "            print(\"current acc\", cur_accuracy)\n",
    "            print(\"best acc\", best_accuracy)\n",
    "            if cur_accuracy > best_accuracy:\n",
    "                print(\"Saving the best model\")\n",
    "                net_name = checkpoint_directory  + '//' +  'resnet50_32_beam'\n",
    "                t.save(net.state_dict(), net_name)  \n",
    "                best_accuracy =  cur_accuracy  \n",
    "            print(\"updated best acc\", best_accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"Saving the predicted value in a csv file\")\n",
    "            file_to_save = f'{save_directory}//topk_pred_beam_val_after_{epoch+1}th_epoch.csv'\n",
    "            indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "            df1 = pd.DataFrame()\n",
    "            df1['index'] = indx                \n",
    "            df1['link_status'] = gt_beam\n",
    "            df1['top1_pred'] = top1_pred_out\n",
    "            df1['top2_pred'] = top2_pred_out\n",
    "            df1['top3_pred'] = top3_pred_out\n",
    "            df1['top5_pred'] = top5_pred_out\n",
    "            df1.to_csv(file_to_save, index=False)                \n",
    "                   \n",
    "            LR_sch.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ee863-3f37-4a91-bac7-bca168af10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################################################################\n",
    "################### Load the model checkpoint ##########################    \n",
    "test_dir = 'scenario23_img_beam_test.csv'\n",
    "checkpoint_path = f'{checkpoint_directory}/resnet50_32_beam'   \n",
    "net.load_state_dict(t.load(checkpoint_path))\n",
    "net.eval() \n",
    "net = net.cuda()   \n",
    "\n",
    "test_loader = DataLoader(DataFeed(test_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False) \n",
    "\n",
    "print('Start validation')\n",
    "ave_top1_acc = 0\n",
    "ave_top2_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top2_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "gt_beam = []\n",
    "total_count = 0\n",
    "for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "    net.eval()\n",
    "    x = imgs.cuda()\n",
    "    opt.zero_grad()\n",
    "    labels = labels.cuda()\n",
    "    total_count += labels.size(0)\n",
    "    _, out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    \n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "    \n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "    \n",
    "    top_2_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:2])\n",
    "    top2_pred_out.append(top_2_pred.detach().cpu().numpy()[0])\n",
    "    \n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "    \n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten)\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])                      \n",
    "    \n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_2_labels = reshaped_labels.repeat(1, 2)\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5) \n",
    "    \n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top2_acc = t.sum(top_2_pred == tiled_2_labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)                    \n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top2_acc += batch_top2_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()                    \n",
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top2_acc.append(ave_top2_acc / total_count)\n",
    "running_top3_acc.append(ave_top3_acc / total_count)  # (batch_size * (count_2 + 1)))\n",
    "running_top5_acc.append(ave_top5_acc / total_count)  # (batch_size * (count_2 + 1)))                \n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-2 accuracy {}'.format( running_top2_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1])) \n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = indx                \n",
    "df2['link_status'] = gt_beam\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top2_pred'] = top2_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2.to_csv(file_to_save, index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
