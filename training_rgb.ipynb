{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba7f2e2-31fd-4d6e-9fc8-a7eace461083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import torch as t\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_feed import DataFeed\n",
    "from build_net import resnet50\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46ac1e4-c6b9-49ad-9f73-d08cdcedcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-31-2024\n",
      "11_25\n",
      "C:\\Users\\aghalkha21\\Downloads\\Git_Projects\\Carla//saved_folder//12-31-2024_11_25\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "########### Create save directory ##########\n",
    "############################################\n",
    "\n",
    "# year month day \n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds \n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)\n",
    "dataset_dir = './image_beam/'\n",
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime \n",
    "print(pwd)\n",
    "# Determine whether the folder already exists\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)    \n",
    "    \n",
    "\n",
    "#copy the training files to the saved directory\n",
    "shutil.copy('./training_rgb.ipynb', pwd)\n",
    "shutil.copy('./data_feed.py', pwd)\n",
    "shutil.copy('./build_net.py', pwd)\n",
    "shutil.copy(dataset_dir + 'scenario23_img_beam_train.csv', pwd)\n",
    "shutil.copy(dataset_dir + 'scenario23_img_beam_val.csv', pwd)\n",
    "shutil.copy(dataset_dir +'scenario23_img_beam_test.csv', pwd)\n",
    "\n",
    "\n",
    "#create folder to save analysis files and checkpoint\n",
    "\n",
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory) \n",
    "    \n",
    "isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)         \n",
    "\n",
    "############################################    \n",
    "\n",
    "########################################################################\n",
    "######################### Hyperparameters ##############################\n",
    "########################################################################\n",
    "\n",
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "lr = 1e-3\n",
    "decay = 1e-4\n",
    "num_epochs = 20\n",
    "train_size = [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d5d937-1175-4b78-9afa-ef5807cd54b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```````````````````````````````````````````````````````\n",
      "Training size is 1\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Epoch No. 1\n",
      "Training-Batch No.10\n",
      "Loss = 1.6428548097610474\n",
      "Training-Batch No.20\n",
      "Loss = 0.8203384280204773\n",
      "Training-Batch No.30\n",
      "Loss = 1.2591379880905151\n",
      "Training-Batch No.40\n",
      "Loss = 0.643446147441864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m _, out \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     63\u001b[0m L \u001b[38;5;241m=\u001b[39m criterion(out, label)\n\u001b[1;32m---> 64\u001b[0m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     66\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dir = dataset_dir + 'scenario23_img_beam_train.csv'\n",
    "val_dir = dataset_dir + 'scenario23_img_beam_val.csv'\n",
    "train_loader = DataLoader(DataFeed(train_dir, transform=proc_pipe),\n",
    "                          batch_size=batch_size,\n",
    "                          #num_workers=8,\n",
    "                          shuffle=False)\n",
    "val_loader = DataLoader(DataFeed(val_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "with cuda.device(-1):\n",
    "   \n",
    "    acc_loss = 0\n",
    "    itr = []\n",
    "    for idx, n in enumerate(train_size):\n",
    "        print('```````````````````````````````````````````````````````')\n",
    "        print('Training size is {}'.format(n))\n",
    "        # Build the network:\n",
    "        net = resnet50(pretrained=True, progress=True, num_classes=64)\n",
    "        net = net#.cuda()\n",
    "        #summary(net, (3, 224, 224)) #.cuda()\n",
    "\n",
    "        #  Optimization parameters:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = optimizer.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [4,8,12], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "        count = 0\n",
    "        running_loss = []\n",
    "        running_top1_acc = []\n",
    "        running_top2_acc = []\n",
    "        running_top3_acc = []\n",
    "        running_top5_acc = []\n",
    "        \n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch No. ' + str(epoch + 1))\n",
    "            skipped_batches = 0\n",
    "            for tr_count, (img, label) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                x = img#.cuda()\n",
    "                opt.zero_grad()\n",
    "                label = label#.cuda()\n",
    "                _, out = net.forward(x)\n",
    "                L = criterion(out, label)\n",
    "                L.backward()\n",
    "                opt.step()\n",
    "                batch_loss = L.item()\n",
    "                acc_loss += batch_loss\n",
    "                count += 1\n",
    "                if np.mod(count, 10) == 0:\n",
    "                    print('Training-Batch No.' + str(count))\n",
    "                    running_loss.append(batch_loss)  # running_loss.append()\n",
    "                    itr.append(count)\n",
    "                    print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "            print('Start validation')\n",
    "            ave_top1_acc = 0\n",
    "            ave_top2_acc = 0\n",
    "            ave_top3_acc = 0\n",
    "            ave_top5_acc = 0\n",
    "            ind_ten = t.as_tensor([0, 1, 2, 3, 4], device='cuda:-1')\n",
    "            top1_pred_out = []\n",
    "            top2_pred_out = []\n",
    "            top3_pred_out = []\n",
    "            top5_pred_out = []\n",
    "            gt_beam = []\n",
    "            total_count = 0\n",
    "            for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "                net.eval()\n",
    "                x = imgs#.cuda()\n",
    "                opt.zero_grad()\n",
    "                labels = labels#.cuda()\n",
    "                total_count += labels.size(0)\n",
    "                _, out = net.forward(x)\n",
    "                _, top_1_pred = t.max(out, dim=1)\n",
    "                \n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "                \n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "                sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "                \n",
    "                top_2_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:2])\n",
    "                top2_pred_out.append(top_2_pred.detach().cpu().numpy()[0])\n",
    "                \n",
    "                top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "                \n",
    "                top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten)\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])                      \n",
    "                \n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_2_labels = reshaped_labels.repeat(1, 2)\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5) \n",
    "                \n",
    "                batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "                batch_top2_acc = t.sum(top_2_pred == tiled_2_labels, dtype=t.float32)\n",
    "                batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "                batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)                    \n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top2_acc += batch_top2_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()                    \n",
    "            print(\"total test examples are\", total_count)\n",
    "            running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "            running_top2_acc.append(ave_top2_acc / total_count)\n",
    "            running_top3_acc.append(ave_top3_acc / total_count)  # (batch_size * (count_2 + 1)))\n",
    "            running_top5_acc.append(ave_top5_acc / total_count)  # (batch_size * (count_2 + 1)))                \n",
    "            print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "            print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "            print('Average Top-2 accuracy {}'.format( running_top2_acc[-1]))\n",
    "            print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "            print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))                \n",
    "\n",
    "            \n",
    "            cur_accuracy  = running_top1_acc[-1]\n",
    "\n",
    "            print(\"current acc\", cur_accuracy)\n",
    "            print(\"best acc\", best_accuracy)\n",
    "            if cur_accuracy > best_accuracy:\n",
    "                print(\"Saving the best model\")\n",
    "                net_name = checkpoint_directory  + '//' +  'resnet50_32_beam'\n",
    "                t.save(net.state_dict(), net_name)  \n",
    "                best_accuracy =  cur_accuracy  \n",
    "            print(\"updated best acc\", best_accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"Saving the predicted value in a csv file\")\n",
    "            file_to_save = f'{save_directory}//topk_pred_beam_val_after_{epoch+1}th_epoch.csv'\n",
    "            indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "            df1 = pd.DataFrame()\n",
    "            df1['index'] = indx                \n",
    "            df1['link_status'] = gt_beam\n",
    "            df1['top1_pred'] = top1_pred_out\n",
    "            df1['top2_pred'] = top2_pred_out\n",
    "            df1['top3_pred'] = top3_pred_out\n",
    "            df1['top5_pred'] = top5_pred_out\n",
    "            df1.to_csv(file_to_save, index=False)                \n",
    "                   \n",
    "            LR_sch.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ee863-3f37-4a91-bac7-bca168af10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################################################################\n",
    "################### Load the model checkpoint ##########################    \n",
    "test_dir = 'scenario23_img_beam_test.csv'\n",
    "checkpoint_path = f'{checkpoint_directory}/resnet50_32_beam'   \n",
    "net.load_state_dict(t.load(checkpoint_path))\n",
    "net.eval() \n",
    "net = net.cuda()   \n",
    "\n",
    "test_loader = DataLoader(DataFeed(test_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False) \n",
    "\n",
    "print('Start validation')\n",
    "ave_top1_acc = 0\n",
    "ave_top2_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top2_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "gt_beam = []\n",
    "total_count = 0\n",
    "for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "    net.eval()\n",
    "    x = imgs.cuda()\n",
    "    opt.zero_grad()\n",
    "    labels = labels.cuda()\n",
    "    total_count += labels.size(0)\n",
    "    _, out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    \n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "    \n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "    \n",
    "    top_2_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:2])\n",
    "    top2_pred_out.append(top_2_pred.detach().cpu().numpy()[0])\n",
    "    \n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "    \n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten)\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])                      \n",
    "    \n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_2_labels = reshaped_labels.repeat(1, 2)\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5) \n",
    "    \n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top2_acc = t.sum(top_2_pred == tiled_2_labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)                    \n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top2_acc += batch_top2_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()                    \n",
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top2_acc.append(ave_top2_acc / total_count)\n",
    "running_top3_acc.append(ave_top3_acc / total_count)  # (batch_size * (count_2 + 1)))\n",
    "running_top5_acc.append(ave_top5_acc / total_count)  # (batch_size * (count_2 + 1)))                \n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-2 accuracy {}'.format( running_top2_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1])) \n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = indx                \n",
    "df2['link_status'] = gt_beam\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top2_pred'] = top2_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2.to_csv(file_to_save, index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
