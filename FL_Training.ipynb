{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29303e79-2191-48c2-88cf-d4f8f97499d9",
   "metadata": {},
   "source": [
    "# Scenario 1: All modalities exists, no heterginity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4f0f1d-2404-4acf-8593-99c905a32a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression, resnet18_mod\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6ca09e-bc32-4f5f-8b1e-f27a82f12f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! PyTorch can use the GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! PyTorch can use the GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9856d27-3d53-43f2-91fa-380f7b7fb8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "lr = 1e-3\n",
    "decay = 1e-4\n",
    "num_epochs = 20\n",
    "train_size = [1]\n",
    "no_users = 20\n",
    "\n",
    "val_losses_stand_alone = []\n",
    "val_losses_FL = []\n",
    "val_losses_SFMTL = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2aff42-b785-4b9f-9c54-59cef3e4af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded_user: 0\n",
      "Loaded_user: 1\n",
      "Loaded_user: 2\n",
      "Loaded_user: 3\n",
      "Loaded_user: 4\n",
      "Loaded_user: 5\n",
      "Loaded_user: 6\n",
      "Loaded_user: 7\n",
      "Loaded_user: 8\n",
      "Loaded_user: 9\n",
      "Loaded_user: 10\n",
      "Loaded_user: 11\n",
      "Loaded_user: 12\n",
      "Loaded_user: 13\n",
      "Loaded_user: 14\n",
      "Loaded_user: 15\n",
      "Loaded_user: 16\n",
      "Loaded_user: 17\n",
      "Loaded_user: 18\n",
      "Loaded_user: 19\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "dataset_dir = \"feature_IID/\"\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "for user_id in range(no_users):\n",
    "    train_dir = dataset_dir + f'user_{user_id}_pos_height_beam_train.csv'\n",
    "    val_dir = dataset_dir + f'user_{user_id}_pos_height_beam_val.csv'\n",
    "    test_dir = dataset_dir + f'user_{user_id}_pos_height_beam_test.csv'\n",
    "    \n",
    "    train_dataset = DataFeed_image_pos(train_dir, transform=proc_pipe)\n",
    "    val_dataset = DataFeed_image_pos(root_dir=val_dir, transform=proc_pipe)\n",
    "    test_dataset = DataFeed_image_pos(root_dir=test_dir, transform=proc_pipe)\n",
    "    \n",
    "    \n",
    "    train_loaders.append(DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                              shuffle=False))\n",
    "    val_loaders.append(DataLoader(val_dataset,\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    test_loaders.append(DataLoader(test_dataset,\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    print(f\"Loaded_user: {user_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d431fabe-5cae-456a-9ecb-10342010679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "for _, (x, y) in zip(range(1), train_loaders[0]):\n",
    "    print(x[\"pos_height\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e09dd8-16e0-4181-949e-8c829098993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n"
     ]
    }
   ],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "user_modalities = [available_modalities for _ in range(no_users)]\n",
    "modality_size = {\"pos_height\": 512, \"images\": 2048}\n",
    "output_sizes = [sum([modality_size[i] for i in user_modality]) for user_modality in user_modalities]\n",
    "\n",
    "local_classifiers = [MultinomialLogisticRegression(output_sizes[i], 64) for i in range(no_users)]\n",
    "for user_id in range(no_users):\n",
    "    user_model = {}\n",
    "    if \"images\" in user_modalities[user_id]:\n",
    "        user_model[\"images\"] = resnet50(pretrained=True, progress=True, num_classes=64).cuda()\n",
    "    if \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model[\"pos_height\"] = NN_beam_pred(num_features=4, num_output=64).cuda()\n",
    "    all_models.append(user_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33e6696-0636-4b3f-bd6e-d84baacd3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix for 'pos_height':\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "Similarity Matrix for 'images':\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store similarity matrices for each modality\n",
    "similarity_matrices = {modality: np.zeros((no_users, no_users), dtype=int) for modality in available_modalities}\n",
    "\n",
    "# Populate the similarity matrices\n",
    "for modality in available_modalities:\n",
    "    for i in range(no_users):\n",
    "        for j in range(no_users):\n",
    "            if modality in user_modalities[i] and modality in user_modalities[j]:\n",
    "                similarity_matrices[modality][i, j] = 1\n",
    "\n",
    "# Print the resulting matrices\n",
    "for modality, matrix in similarity_matrices.items():\n",
    "    print(f\"Similarity Matrix for '{modality}':\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f16a6d65-dc6d-4352-a597-d37acd1554ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        if nx.is_connected(graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1345c76-333e-4180-b438-4466eed22669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random connected topology\n",
    "adjacency_matrix = create_random_topology(no_users, edge_probability=0.3)\n",
    "\n",
    "# Prepare mixing matrices for each modality\n",
    "mixing_matrices, adjacency_matrices = prepare_mixing_matrices(adjacency_matrix, similarity_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093577f9-b3f0-4713-91b9-ef34f2ab76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Decentralized aggregation function\n",
    "def decentralized_aggregation(user_models, mixing_matrices, available_modalities):\n",
    "    num_users = len(user_models)\n",
    "    \n",
    "    for modality in available_modalities:\n",
    "        # Get the mixing matrix for the current modality\n",
    "        mixing_matrix = mixing_matrices[modality]\n",
    "        \n",
    "        # Convert user model parameters to vectors for aggregation\n",
    "        aggregated_models = [torch.nn.utils.parameters_to_vector(user_model[modality].parameters()) for user_model in user_models]\n",
    "        \n",
    "        # Initialize aggregated updates\n",
    "        aggregated_updates = [torch.zeros_like(aggregated_models[0]) for _ in range(num_users)]\n",
    "        \n",
    "        # Perform model aggregation based on the mixing matrix for this modality\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_users):\n",
    "                if mixing_matrix[i, j] > 0:\n",
    "                    aggregated_updates[i] += mixing_matrix[i, j] * aggregated_models[j]\n",
    "        \n",
    "        # Update user models with aggregated parameters for the current modality\n",
    "        for i in range(num_users):\n",
    "            torch.nn.utils.vector_to_parameters(aggregated_updates[i], user_models[i][modality].parameters())\n",
    "\n",
    "\n",
    "def train_local_model(local_modalities, models, train_loader, criterion, optimizers, epochs):\n",
    "\n",
    "    for modality in local_modalities:\n",
    "        print(f\"Training for modality: {modality}\")\n",
    "        \n",
    "        model = models[modality]\n",
    "        optimizer = optimizers[modality]\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for data, labels in train_loader:\n",
    "                # Move data to GPU if available\n",
    "                data = data[modality]\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                temp, outputs = model(data)\n",
    "                print(temp.shape)\n",
    "\n",
    "               \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Optional: Print loss for debugging\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Modality: {modality}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a079548-3c7c-46bb-bfc1-be6ad5984a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_models(user_id, user_models, val_loaders, criterion):\n",
    "\n",
    "    print(f\"Validating model for User {user_id + 1}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for modality, model in user_models.items():\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            model.eval()\n",
    "            if modality not in user_models.keys():\n",
    "                print(f\"Skipping modality {modality} for User {user_id + 1}, no validation data.\")\n",
    "                continue\n",
    "            \n",
    "            for data, labels in val_loaders:  # Iterate over validation data for the modality\n",
    "                data = data[modality]\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                # Forward pass\n",
    "                _, outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate loss and accuracy\n",
    "                total_loss += loss.item() * labels.size(0)  # Sum loss for the batch\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "            accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "            print(f\"User {user_id + 1}, modality: {modality} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return {'loss': avg_loss, 'accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e73c81-b91c-4bf3-9943-c8c6b50cc77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round 1\n",
      "Training image modalitity models\n",
      "Training model for User 1\n",
      "Training for modality: pos_height\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1766\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1608\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1276\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1209\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.1139\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0743\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0565\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 4.0252\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.9783\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.9705\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.9210\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.8806\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.7855\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.7209\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.7232\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.6010\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.4969\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.4071\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.3140\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.3182\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.1499\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.1258\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.2455\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 2.8398\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 2.6260\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 2.9260\n",
      "torch.Size([50, 128])\n",
      "Epoch [1/1], Modality: pos_height, Loss: 3.0300\n",
      "Training for modality: images\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 4.4538\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 3.9409\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 3.3294\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 4.0411\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 3.7195\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 3.0013\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.6621\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.6551\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.1928\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.7134\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.0794\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.5870\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.4440\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.9660\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.2176\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.8564\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.7679\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.5937\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 2.1422\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.4899\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.4068\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.5840\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.9477\n",
      "torch.Size([64, 128])\n",
      "Epoch [1/1], Modality: images, Loss: 1.7027\n",
      "torch.Size([64, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m modality \u001b[38;5;129;01min\u001b[39;00m user_model\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     18\u001b[0m         optimizers[modality] \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(user_models[modality]\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mtrain_local_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_modalities\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Decentralized aggregation\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming decentralized aggregation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m, in \u001b[0;36mtrain_local_model\u001b[1;34m(local_modalities, models, train_loader, criterion, optimizers, epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;66;03m# Optional: Print loss for debugging\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Modality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "local_epochs = 1\n",
    "global_rounds = 10\n",
    "weight_decay = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decentralized Training\n",
    "for round_num in range(global_rounds):\n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Training for image_modalities \n",
    "    print(\"Training image modalitity models\")\n",
    "    # Train each user's local model\n",
    "    for user_id in range(no_users):\n",
    "        print(f\"Training model for User {user_id + 1}\")\n",
    "        user_models = all_models[user_id]\n",
    "        optimizers = {}\n",
    "        for modality in user_model.keys():\n",
    "            optimizers[modality] = optim.Adam(user_models[modality].parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train_local_model(user_modalities[user_id], user_models, train_loaders[user_id], criterion, optimizers, local_epochs)\n",
    "\n",
    "    # Decentralized aggregation\n",
    "    print(\"Performing decentralized aggregation\")\n",
    "    #decentralized_aggregation(user_models, mixing_matrix)\n",
    "\n",
    "    # Optionally, validate models\n",
    "    print(\"Validating user models...\")\n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        val_dict = validate_user_models(user_id, user_models, val_loaders[user_id], criterion)\n",
    "        val_losses_stand_alone.append(val_dict)\n",
    "\n",
    "\n",
    "print(\"Decentralized federated learning complete.\")\n",
    "\n",
    "file_path = 'val_losses_stand_alone.json'\n",
    "\n",
    "# Save the list of dictionaries to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(val_losses_stand_alone, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be75bb1-af67-4841-8422-434bddf7913f",
   "metadata": {},
   "source": [
    "# Decentralized FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ca8a8-151f-487e-b619-80692ac78789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "user_modalities = [available_modalities for _ in range(no_users)]\n",
    "modality_size = {\"pos_height\": 512, \"images\": 2048}\n",
    "output_sizes = [sum([modality_size[i] for i in user_modality]) for user_modality in user_modalities]\n",
    "\n",
    "local_classifiers = [MultinomialLogisticRegression(output_sizes[i], 64) for i in range(no_users)]\n",
    "for user_id in range(no_users):\n",
    "    user_model = {}\n",
    "    if \"images\" in user_modalities[user_id]:\n",
    "        user_model[\"images\"] = resnet50(pretrained=True, progress=True, num_classes=64).cuda()\n",
    "    if \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model[\"pos_height\"] = NN_beam_pred(num_features=4, num_output=64).cuda()\n",
    "    all_models.append(user_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fb99571-7241-4590-bf2c-5cb0bc13cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round 1\n",
      "Training image modalitity models\n",
      "Training model for User 1\n",
      "Performing decentralized aggregation\n",
      "Validating user models...\n",
      "Validating model for User 1\n",
      "User 1, modality: images - Validation Loss: 4.1566, Accuracy: 0.0000\n",
      "User 1, modality: pos_height - Validation Loss: 4.1579, Accuracy: 0.0117\n",
      "Global Round 2\n",
      "Training image modalitity models\n",
      "Training model for User 1\n",
      "Performing decentralized aggregation\n",
      "Validating user models...\n",
      "Validating model for User 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     28\u001b[0m         user_models \u001b[38;5;241m=\u001b[39m all_models[user_id]\n\u001b[1;32m---> 29\u001b[0m         val_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_user_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m         val_losses\u001b[38;5;241m.\u001b[39mappend(val_dict)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecentralized federated learning complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m, in \u001b[0;36mvalidate_user_models\u001b[1;34m(user_id, user_models, val_loaders, criterion)\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Accumulate loss and accuracy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Sum loss for the batch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "local_epochs = 1\n",
    "global_rounds = 10\n",
    "weight_decay = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Decentralized Training\n",
    "for round_num in range(global_rounds):\n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Training for image_modalities \n",
    "    print(\"Training image modalitity models\")\n",
    "    # Train each user's local model\n",
    "    for user_id in range(no_users):\n",
    "        print(f\"Training model for User {user_id + 1}\")\n",
    "        user_models = all_models[user_id]\n",
    "        optimizers = {}\n",
    "        for modality in user_model.keys():\n",
    "            optimizers[modality] = optim.Adam(user_models[modality].parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #train_local_model(user_modalities[user_id], user_models, train_loaders[user_id], criterion, optimizers, local_epochs)\n",
    "\n",
    "    # Decentralized aggregation\n",
    "    print(\"Performing decentralized aggregation\")\n",
    "    decentralized_aggregation(all_models, mixing_matrices, available_modalities)\n",
    "\n",
    "    # Optionally, validate models\n",
    "    print(\"Validating user models...\")\n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        val_dict = validate_user_models(user_id, user_models, val_loaders[user_id], criterion)\n",
    "        val_losses_FL.append(val_dict)\n",
    "\n",
    "\n",
    "print(\"Decentralized federated learning complete.\")\n",
    "\n",
    "file_path = 'val_losses_FL.json'\n",
    "\n",
    "# Save the list of dictionaries to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(val_losses_FL, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add479e-d5e9-4ca5-a38e-8ef4e3fa9a5b",
   "metadata": {},
   "source": [
    "# Sheaf_assisted FML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c641d1c9-67ce-462c-bd51-65e7d4b5962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n"
     ]
    }
   ],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "user_modalities = [available_modalities for _ in range(no_users)]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "output_sizes = [sum([modality_size[i] for i in user_modality]) for user_modality in user_modalities]\n",
    "\n",
    "local_classifiers = [MultinomialLogisticRegression(output_sizes[i], 64).cuda() for i in range(no_users)]\n",
    "for user_id in range(no_users):\n",
    "    user_model = {}\n",
    "    if \"images\" in user_modalities[user_id]:\n",
    "        user_model[\"images\"] = resnet50(pretrained=True, progress=True, num_classes=64).cuda()\n",
    "    if \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model[\"pos_height\"] = NN_beam_pred(num_features=4, num_output=64).cuda()\n",
    "    all_models.append(user_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "feb1fd09-c62d-466a-b3c2-7bfc917594da",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 0.05\n",
    "neighbors = [np.nonzero(adjacency_matrix[i])[0].tolist() for i in range(no_users)]\n",
    "\n",
    "# Initialize P_ij matrices\n",
    "# Initialize P_ij matrices\n",
    "P = {}\n",
    "for i, j in zip(*adjacency_matrix.nonzero()):\n",
    "    num_params_i = sum(p.numel() for p in local_classifiers[i].parameters())\n",
    "    num_params_j = sum(p.numel() for p in local_classifiers[j].parameters())\n",
    "    P[(i, j)] = torch.randn(int(factor*(num_params_i + num_params_j) // 2), num_params_i).cuda()\n",
    "    P[(j, i)] = torch.randn(int(factor*(num_params_i + num_params_j) // 2), num_params_j).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90f2f12b-710a-4c11-8fb9-a1db91beff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_model_FMTL(user_id, user_models, client_model, val_loader, loss_func):\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        # Set all models to evaluation mode\n",
    "        client_model.eval()\n",
    "        for model in user_models.values():\n",
    "            model.eval()\n",
    "\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            # Concatenate outputs from all modality models\n",
    "            modality_outputs = []\n",
    "            y_batch = y_batch.cuda()\n",
    "\n",
    "            for modality, model in user_models.items():\n",
    "\n",
    "                # Forward pass through modality model\n",
    "                data = X_batch[modality].cuda()\n",
    "                modality_output, _ = model(data)\n",
    "                modality_outputs.append(modality_output)\n",
    "\n",
    "            # Skip if no valid modalities are present\n",
    "            if len(modality_outputs) == 0:\n",
    "                print(f\"No valid modalities for User {user_id + 1}, skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            # Concatenate all modality outputs\n",
    "            concatenated_output = torch.cat(modality_outputs, dim=1)\n",
    "\n",
    "            # Forward pass through the classifier\n",
    "            predictions = client_model(concatenated_output)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_func(client_model, concatenated_output, y_batch, l2_strength=0.01)\n",
    "            total_loss += loss.item() * y_batch.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            total_correct += (predicted == y_batch).sum().item()\n",
    "            total_samples += y_batch.size(0)\n",
    "\n",
    "    # Compute overall metrics\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    print(f\"User {user_id + 1} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return {'loss': avg_loss, 'accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42422a44-c56d-49ae-9487-b7c0f7466c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round 1\n",
      "Training image modalitity models\n",
      "Performing FMTL aggregation\n",
      "Validating user models...\n",
      "User 1 - Validation Loss: 4.4811, Accuracy: 0.0000\n",
      "User 2 - Validation Loss: 4.7501, Accuracy: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m         user_models \u001b[38;5;241m=\u001b[39m all_models[user_id]\n\u001b[0;32m    102\u001b[0m         client_model \u001b[38;5;241m=\u001b[39m local_classifiers[i]\n\u001b[1;32m--> 103\u001b[0m         val_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_user_model_FMTL\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m         val_losses_SFMTL\u001b[38;5;241m.\u001b[39mappend(val_dict)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecentralized federated learning complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[63], line 15\u001b[0m, in \u001b[0;36mvalidate_user_model_FMTL\u001b[1;34m(user_id, user_models, client_model, val_loader, loss_func)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m user_models\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Concatenate outputs from all modality models\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     modality_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Desktop\\Git_Projects\\Carla\\data_feed.py:81\u001b[0m, in \u001b[0;36mDataFeed_image_pos.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     79\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n\u001b[0;32m     80\u001b[0m pos \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 81\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m     83\u001b[0m label \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\skimage\\io\\_io.py:60\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     57\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname:\n\u001b[1;32m---> 60\u001b[0m     img \u001b[38;5;241m=\u001b[39m call_plugin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimread\u001b[39m\u001b[38;5;124m'\u001b[39m, fname, plugin\u001b[38;5;241m=\u001b[39mplugin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\skimage\\io\\manage_plugins.py:217\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(imageio_imread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWRITEABLE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     13\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\imageio\\v3.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_kwargs) \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(img_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\imageio\\core\\imopen.py:113\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request\u001b[38;5;241m.\u001b[39mformat_hint \u001b[38;5;241m=\u001b[39m format_hint\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bytes>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\imageio\\core\\request.py:247\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\site-packages\\imageio\\core\\request.py:381\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip:\n\u001b[0;32m    380\u001b[0m         fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_zip[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m (fn \u001b[38;5;129;01min\u001b[39;00m EXAMPLE_IMAGES):\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. This file looks like one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe standard images, but from imageio 2.1, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard images have to be specified using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageio:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (fn, fn)\n\u001b[0;32m    387\u001b[0m         )\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Make filename absolute\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sionna_env\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import cross_entropy_loss_with_l2\n",
    "local_epochs = 1\n",
    "global_rounds = 10\n",
    "weight_decay = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_func = cross_entropy_loss_with_l2\n",
    "alpha = 0.01\n",
    "lambda_reg = 0.01\n",
    "eta = 0.01\n",
    "\n",
    "# Decentralized Training\n",
    "for round_num in range(global_rounds):\n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Training for image_modalities \n",
    "    print(\"Training image modalitity models\")\n",
    "    # Train each user's local model\n",
    "    for user_id in range(no_users):\n",
    "        print(f\"Training model for User {user_id + 1}\")\n",
    "        user_models = all_models[user_id]\n",
    "        optimizers = {}\n",
    "        for modality in user_model.keys():\n",
    "            optimizers[modality] = optim.Adam(user_models[modality].parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train_local_model(user_modalities[user_id], user_models, train_loaders[user_id], criterion, optimizers, local_epochs)\n",
    "\n",
    "    # FMTL Training Part\n",
    "    print(\"Performing FMTL aggregation\")\n",
    "    for i in range(no_users):\n",
    "        client_model = local_classifiers[i]\n",
    "        user_models = all_models[user_id]\n",
    "        optimizer = optim.Adam(client_model.parameters(), lr=alpha, weight_decay=weight_decay)\n",
    "        \n",
    "        # Training the model\n",
    "        client_model.train()\n",
    "        for X_batch, y_batch in train_loaders[i]:\n",
    "            # Ensure all modality models are in evaluation mode\n",
    "            for modality, model in user_models.items():\n",
    "                model.eval()\n",
    "        \n",
    "            # Concatenate outputs from all modality models\n",
    "            modality_outputs = []\n",
    "            for modality, model in user_models.items():\n",
    "                with torch.no_grad():  # Do not compute gradients for modality models \n",
    "                    data, y_batch = X_batch[modality].cuda(), y_batch.cuda()\n",
    "                    modality_output, _ = model(data)\n",
    "                    modality_outputs.append(modality_output)\n",
    "\n",
    "        \n",
    "            # Concatenate all modality outputs\n",
    "            concatenated_output = torch.cat(modality_outputs, dim=1)\n",
    "\n",
    "            # Forward pass through the classifier\n",
    "            optimizer.zero_grad()\n",
    "            predictions = client_model(concatenated_output)\n",
    "            \n",
    "                    \n",
    "            # Compute the loss\n",
    "            loss = loss_func(client_model, concatenated_output, y_batch, l2_strength=0.01)\n",
    "        \n",
    "            # Backward pass and optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            theta_i = torch.cat([param.view(-1) for param in client_model.parameters()])\n",
    "    \n",
    "            # Update theta_i based on P_ij and neighbors\n",
    "            sum_P_terms = torch.zeros_like(theta_i)\n",
    "            for j in range(no_users):\n",
    "                if adjacency_matrix[i, j] == 1:\n",
    "                    P_ij = P[(i, j)]\n",
    "                    P_ji = P[(j, i)]\n",
    "                    theta_j = torch.cat([param.view(-1) for param in local_classifiers[j].parameters()])\n",
    "                    sum_P_terms += P_ij.T @ (P_ij @ theta_i - P_ji @ theta_j)\n",
    "    \n",
    "            # Apply update to theta_i\n",
    "            theta_i -= alpha * lambda_reg * sum_P_terms\n",
    "    \n",
    "            # Put updated theta_i back into the model\n",
    "            idx = 0\n",
    "            for param in client_model.parameters():\n",
    "                numel = param.numel()\n",
    "                param.data.copy_(theta_i[idx:idx+numel].reshape(param.size()))\n",
    "                idx += numel\n",
    "    \n",
    "            # Update P_ij matrices\n",
    "            for j in range(no_users):\n",
    "                if adjacency_matrix[i, j] == 1:\n",
    "                    P_ij = P[(i, j)]\n",
    "                    P_ji = P[(j, i)]\n",
    "                    theta_j = torch.cat([param.view(-1) for param in local_classifiers[j].parameters()])\n",
    "    \n",
    "                    # Update P_ij\n",
    "                    P[(i, j)] -= eta * lambda_reg * torch.outer(P_ij @ theta_i - P_ji @ theta_j, theta_i)\n",
    "        \n",
    "\n",
    "    # Optionally, validate models\n",
    "    print(\"Validating user models...\")\n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        client_model = local_classifiers[i]\n",
    "        val_dict = validate_user_model_FMTL(user_id, user_models, client_model, val_loaders[user_id], loss_func)\n",
    "        val_losses_SFMTL.append(val_dict)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Decentralized federated learning complete.\")\n",
    "\n",
    "file_path = 'val_losses_SFMTL.json'\n",
    "\n",
    "# Save the list of dictionaries to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(val_losses_SFMTL, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c2fc9-8e70-4b8a-b028-5a12a82cb7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
